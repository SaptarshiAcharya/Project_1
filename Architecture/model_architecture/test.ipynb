{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8b6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.modules import (\n",
    "    AIFI,\n",
    "    C1,\n",
    "    C2,\n",
    "    C2PSA,\n",
    "    C3,\n",
    "    C3TR,\n",
    "    ELAN1,\n",
    "    OBB,\n",
    "    PSA,\n",
    "    SPP,\n",
    "    SPPELAN,\n",
    "    # SPPF,\n",
    "    # A2C2f,\n",
    "    AConv,\n",
    "    ADown,\n",
    "    # Bottleneck,\n",
    "    BottleneckCSP,\n",
    "    # C2f,\n",
    "    C2fAttn,\n",
    "    C2fCIB,\n",
    "    C2fPSA,\n",
    "    C3Ghost,\n",
    "    C3k2,\n",
    "    C3x,\n",
    "    CBFuse,\n",
    "    CBLinear,\n",
    "    Classify,\n",
    "    Concat,\n",
    "    # Conv,\n",
    "    Conv2,\n",
    "    ConvTranspose,\n",
    "    Detect,\n",
    "    DWConv,\n",
    "    DWConvTranspose2d,\n",
    "    Focus,\n",
    "    GhostBottleneck,\n",
    "    GhostConv,\n",
    "    HGBlock,\n",
    "    HGStem,\n",
    "    ImagePoolingAttn,\n",
    "    # Index,\n",
    "    # LRPCHead,\n",
    "    Pose,\n",
    "    RepC3,\n",
    "    RepConv,\n",
    "    RepNCSPELAN4,\n",
    "    RepVGGDW,\n",
    "    ResNetLayer,\n",
    "    RTDETRDecoder,\n",
    "    SCDown,\n",
    "    Segment,\n",
    "    # TorchVision,\n",
    "    WorldDetect,\n",
    "    # YOLOEDetect,\n",
    "    # YOLOESegment,\n",
    "    v10Detect,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fbafb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bottleneck', 'C2f', 'Concat', 'Conv', 'DFL', 'Head', 'SPPF', 'Upsample', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'cv2', 'math', 'nn', 'np', 'os', 'random', 'torch', 'yolo_params']\n"
     ]
    }
   ],
   "source": [
    "import model_classes\n",
    "print(dir(model_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6321a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_classes import(\n",
    "    Conv,\n",
    "    C2f,\n",
    "    SPPF,\n",
    "    Upsample,\n",
    "    DFL,\n",
    "    Head,\n",
    "    # meow_test,\n",
    "    Bottleneck,\n",
    "    Concat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74f9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, yaml_load as YAML, colorstr, emojis\n",
    "from ultralytics.utils.checks import check_requirements, check_suffix, check_yaml\n",
    "import torch\n",
    "import torch.nn as n\n",
    "import contextlib\n",
    "from ultralytics.utils.ops import make_divisible\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c84fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model_scale(model_path):\n",
    "    \"\"\"\n",
    "    Extract the size character n, s, m, l, or x of the model's scale from the model path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str | Path): The path to the YOLO model's YAML file.\n",
    "\n",
    "    Returns:\n",
    "        (str): The size character of the model's scale (n, s, m, l, or x).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return re.search(r\"yolo(e-)?[v]?\\d+([nslmx])\", Path(model_path).stem).group(2)  # noqa\n",
    "    except AttributeError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9135469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "print(ultralytics.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c8934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaml_model_load(path):\n",
    "    \"\"\"\n",
    "    Load a YOLOv8 model from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Model dictionary.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if path.stem in (f\"yolov{d}{x}6\" for x in \"nsmlx\" for d in (5, 8)):\n",
    "        new_stem = re.sub(r\"(\\d+)([nslmx])6(.+)?$\", r\"\\1\\2-p6\\3\", path.stem)\n",
    "        LOGGER.warning(f\"Ultralytics YOLO P6 models now use -p6 suffix. Renaming {path.stem} to {new_stem}.\")\n",
    "        path = path.with_name(new_stem + path.suffix)\n",
    "\n",
    "    unified_path = re.sub(r\"(\\d+)([nslmx])(.+)?$\", r\"\\1\\3\", str(path))  # i.e. yolov8x.yaml -> yolov8.yaml\n",
    "    yaml_file = check_yaml(unified_path, hard=False) or check_yaml(path)\n",
    "    d = YAML(yaml_file)  # model dict\n",
    "    d[\"scale\"] = guess_model_scale(path)\n",
    "    d[\"yaml_file\"] = str(path)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658c6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_model(d, ch, verbose=True):\n",
    "    \"\"\"\n",
    "    Parse a YOLO model.yaml dictionary into a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        d (dict): Model dictionary.\n",
    "        ch (int): Input channels.\n",
    "        verbose (bool): Whether to print model details.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Sequential): PyTorch model.\n",
    "        save (list): Sorted list of output layers.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "\n",
    "    # Args\n",
    "    legacy = True  # backward compatibility for v3/v5/v8/v9 models\n",
    "    max_channels = float(\"inf\")\n",
    "    nc, act, scales = (d.get(x) for x in (\"nc\", \"activation\", \"scales\"))\n",
    "    depth, width, kpt_shape = (d.get(x, 1.0) for x in (\"depth_multiple\", \"width_multiple\", \"kpt_shape\"))\n",
    "    if scales:\n",
    "        scale = d.get(\"scale\")\n",
    "        if not scale:\n",
    "            scale = tuple(scales.keys())[0]\n",
    "            LOGGER.warning(f\"no model scale passed. Assuming scale='{scale}'.\")\n",
    "        depth, width, max_channels = scales[scale]\n",
    "\n",
    "    if act:\n",
    "        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = torch.nn.SiLU()\n",
    "        if verbose:\n",
    "            LOGGER.info(f\"{colorstr('activation:')} {act}\")  # print\n",
    "\n",
    "    if verbose:\n",
    "        LOGGER.info(f\"\\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}\")\n",
    "    ch = [ch]\n",
    "    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
    "    base_modules = frozenset(\n",
    "        {\n",
    "            Classify,\n",
    "            Conv,\n",
    "            ConvTranspose,\n",
    "            GhostConv,\n",
    "            Bottleneck,\n",
    "            GhostBottleneck,\n",
    "            SPP,\n",
    "            SPPF,\n",
    "            C2fPSA,\n",
    "            C2PSA,\n",
    "            DWConv,\n",
    "            Focus,\n",
    "            BottleneckCSP,\n",
    "            C1,\n",
    "            C2,\n",
    "            C2f,\n",
    "            C3k2,\n",
    "            RepNCSPELAN4,\n",
    "            ELAN1,\n",
    "            ADown,\n",
    "            AConv,\n",
    "            SPPELAN,\n",
    "            C2fAttn,\n",
    "            C3,\n",
    "            C3TR,\n",
    "            C3Ghost,\n",
    "            # torch.nn.ConvTranspose2d,\n",
    "            DWConvTranspose2d,\n",
    "            C3x,\n",
    "            RepC3,\n",
    "            PSA,\n",
    "            SCDown,\n",
    "            C2fCIB,\n",
    "            # A2C2f,\n",
    "        }\n",
    "    )\n",
    "    repeat_modules = frozenset(  # modules with 'repeat' arguments\n",
    "        {\n",
    "            BottleneckCSP,\n",
    "            C1,\n",
    "            C2,\n",
    "            C2f,\n",
    "            C3k2,\n",
    "            C2fAttn,\n",
    "            C3,\n",
    "            C3TR,\n",
    "            C3Ghost,\n",
    "            C3x,\n",
    "            RepC3,\n",
    "            C2fPSA,\n",
    "            C2fCIB,\n",
    "            C2PSA,\n",
    "            # A2C2f,\n",
    "        }\n",
    "    )\n",
    "    for i, (f, n, m, args) in enumerate(d[\"backbone\"] + d[\"head\"]):  # from, number, module, args\n",
    "        m = (\n",
    "            getattr(torch.nn, m[3:])\n",
    "            if \"nn.\" in m\n",
    "            else getattr(__import__(\"torchvision\").ops, m[16:])\n",
    "            if \"torchvision.ops.\" in m\n",
    "            else globals()[m]\n",
    "        )  # get module\n",
    "        for j, a in enumerate(args):\n",
    "            if isinstance(a, str):\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)\n",
    "        n = n_ = max(round(n * depth), 1) if n > 1 else n  # depth gain\n",
    "        if m in base_modules:\n",
    "            c1, c2 = ch[f], args[0]\n",
    "            if c2 != nc:  # if c2 not equal to number of classes (i.e. for Classify() output)\n",
    "                c2 = make_divisible(min(c2, max_channels) * width, 8)\n",
    "            if m is C2fAttn:  # set 1) embed channels and 2) num heads\n",
    "                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)\n",
    "                args[2] = int(max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2])\n",
    "\n",
    "            args = [c1, c2, *args[1:]]\n",
    "            if m in repeat_modules:\n",
    "                args.insert(2, n)  # number of repeats\n",
    "                n = 1\n",
    "            if m is C3k2:  # for M/L/X sizes\n",
    "                legacy = False\n",
    "                if scale in \"mlx\":\n",
    "                    args[3] = True\n",
    "            # if m is A2C2f:\n",
    "            #     legacy = False\n",
    "            #     if scale in \"lx\":  # for L/X sizes\n",
    "            #         args.extend((True, 1.2))\n",
    "            if m is C2fCIB:\n",
    "                legacy = False\n",
    "        elif m is AIFI:\n",
    "            args = [ch[f], *args]\n",
    "        elif m in frozenset({HGStem, HGBlock}):\n",
    "            c1, cm, c2 = ch[f], args[0], args[1]\n",
    "            args = [c1, cm, c2, *args[2:]]\n",
    "            if m is HGBlock:\n",
    "                args.insert(4, n)  # number of repeats\n",
    "                n = 1\n",
    "        elif m is ResNetLayer:\n",
    "            c2 = args[1] if args[3] else args[1] * 4\n",
    "        elif m is torch.nn.BatchNorm2d:\n",
    "            args = [ch[f]]\n",
    "        elif m is Concat:\n",
    "            c2 = sum(ch[x] for x in f)\n",
    "        # elif m in frozenset(\n",
    "        #     {Detect, WorldDetect, YOLOEDetect, Segment, YOLOESegment, Pose, OBB, ImagePoolingAttn, v10Detect}\n",
    "        # ):\n",
    "        #     args.append([ch[x] for x in f])\n",
    "        #     if m is Segment or m is YOLOESegment:\n",
    "        #         args[2] = make_divisible(min(args[2], max_channels) * width, 8)\n",
    "        #     if m in {Detect, YOLOEDetect, Segment, YOLOESegment, Pose, OBB}:\n",
    "        #         m.legacy = legacy\n",
    "        elif m in frozenset({Detect,Head}):\n",
    "            args.append([ch[x] for x in f])\n",
    "            if m in {Detect,Head}:\n",
    "                m.legacy = legacy\n",
    "        elif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1\n",
    "            args.insert(1, [ch[x] for x in f])\n",
    "        elif m is CBLinear:\n",
    "            c2 = args[0]\n",
    "            c1 = ch[f]\n",
    "            args = [c1, c2, *args[1:]]\n",
    "        elif m is CBFuse:\n",
    "            c2 = ch[f[-1]]\n",
    "        # elif m in frozenset({TorchVision, Index}):\n",
    "        #     c2 = args[0]\n",
    "        #     c1 = ch[f]\n",
    "        #     args = [*args[1:]]\n",
    "        else:\n",
    "            c2 = ch[f]\n",
    "        m_ = torch.nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n",
    "        t = str(m)[8:-2].replace(\"__main__.\", \"\")  # module type\n",
    "        m_.np = sum(x.numel() for x in m_.parameters())  # number params\n",
    "        m_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type\n",
    "        if verbose:\n",
    "            LOGGER.info(f\"{i:>3}{str(f):>20}{n_:>3}{m_.np:10.0f}  {t:<45}{str(args):<30}\")  # print\n",
    "        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n",
    "        layers.append(m_)\n",
    "        if i == 0:\n",
    "            ch = []\n",
    "        ch.append(c2)\n",
    "    return torch.nn.Sequential(*layers), sorted(save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67257869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_yaml= \"my_model.yaml\"\n",
    "# mod_ya = model_yaml if isinstance(model_yaml, dict) else yaml_model_load(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebaa6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nc = 80  # replace with your actual number of classes\n",
    "# ch = 3\n",
    "# yaml_dict = mod_ya.copy()\n",
    "# yaml_dict['backbone'] = [\n",
    "#     [-1, 1, 'Conv', [64]],  # 0\n",
    "#     [-1, 1, 'Conv', [128]], # 1\n",
    "#     [-1, 1, 'Conv', [256]]  # 2\n",
    "# ]\n",
    "\n",
    "# yaml_dict['head'] = [\n",
    "#     [[0, 1, 2], 1, 'Detect', [nc]]  # 3 inputs for Detect\n",
    "# ]\n",
    "# # Step 3: Parse it into a model\n",
    "# model, _ = parse_model(deepcopy(yaml_dict), ch=ch, verbose=True)\n",
    "# print(\"\\n\\nParsed Model:\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca66ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.model_classes.Conv            [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.model_classes.Conv            [16, 32, 3, 2]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2                  -1  1      7360  ultralytics.nn.model_classes.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.model_classes.Conv            [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.model_classes.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.model_classes.Conv            [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.model_classes.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.model_classes.Conv            [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.model_classes.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.model_classes.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  ultralytics.nn.model_classes.Upsample        [2, 'nearest']                \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.model_classes.Concat          [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.model_classes.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  ultralytics.nn.model_classes.Upsample        [2, 'nearest']                \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.model_classes.Concat          [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.model_classes.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.model_classes.Conv            [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.model_classes.Concat          [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.model_classes.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.model_classes.Conv            [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.model_classes.Concat          [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.model_classes.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n"
     ]
    }
   ],
   "source": [
    "ch=3\n",
    "verbose = True\n",
    "model_yaml= \"my_model.yaml\"\n",
    "mod_ya = model_yaml if isinstance(model_yaml, dict) else yaml_model_load(model_yaml)\n",
    "# model, save = parse_model(deepcopy(mod_ya), ch=ch, verbose=verbose)\n",
    "from ultralytics.nn.tasks import parse_model\n",
    "model, save  = parse_model(deepcopy(mod_ya), ch=ch, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a97e21ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.134 is required but found version=8.3.28, to fix: `pip install ultralytics==8.0.134`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in C:\\Users\\KIIT\\OneDrive\\Desktop\\Vedant_Official\\vedant projects and works\\ML_Deep_learning_projects\\Deep_Learning_Projects\\Project_1\\Dataset to yolov8:: 100%|██████████| 44579/44579 [00:55<00:00, 796.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to C:\\Users\\KIIT\\OneDrive\\Desktop\\Vedant_Official\\vedant projects and works\\ML_Deep_learning_projects\\Deep_Learning_Projects\\Project_1\\Dataset in yolov8::   1%|          | 12/1396 [00:00<00:01, 819.68it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\KIIT\\\\OneDrive\\\\Desktop\\\\Vedant_Official\\\\vedant projects and works\\\\ML_Deep_learning_projects\\\\Deep_Learning_Projects\\\\Project_1\\\\Dataset\\\\test\\\\images\\\\Poachers20with20gun20prowling20in20Bannerghatta-1631912935-1587208022JPG_jpg.rf.21b5c4254f11de9f1bee87b699100d75.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m version \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKIIT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVedant_Official\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvedant projects and works\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mML_Deep_learning_projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeep_Learning_Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\roboflow\\core\\version.py:237\u001b[0m, in \u001b[0;36mVersion.download\u001b[1;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[0;32m    234\u001b[0m             response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__download_zip(link, location, model_format)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extract_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reformat_yaml(location, model_format)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion, model_format, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(location))\n",
      "File \u001b[1;32mc:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\roboflow\\core\\version.py:696\u001b[0m, in \u001b[0;36mVersion.__extract_zip\u001b[1;34m(self, location, format)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    692\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39minfolist(),\n\u001b[0;32m    693\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting Dataset Version Zip to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    694\u001b[0m ):\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 696\u001b[0m         \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError unzipping download\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1662\u001b[0m, in \u001b[0;36mZipFile.extract\u001b[1;34m(self, member, path, pwd)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1660\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1733\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1729\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(targetpath)\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m-> 1733\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m   1734\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(source, target)\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\KIIT\\\\OneDrive\\\\Desktop\\\\Vedant_Official\\\\vedant projects and works\\\\ML_Deep_learning_projects\\\\Deep_Learning_Projects\\\\Project_1\\\\Dataset\\\\test\\\\images\\\\Poachers20with20gun20prowling20in20Bannerghatta-1631912935-1587208022JPG_jpg.rf.21b5c4254f11de9f1bee87b699100d75.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"u8Ur5ywKLswqwt0uMTXl\")\n",
    "project = rf.workspace(\"poacher-ifyuf\").project(\"poacher\")\n",
    "version = project.version(2)\n",
    "path = r\"C:\\Users\\KIIT\\OneDrive\\Desktop\\Vedant_Official\\vedant projects and works\\ML_Deep_learning_projects\\Deep_Learning_Projects\\Project_1\\Dataset\"\n",
    "dataset = version.download(\"yolov8\", location=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import argparse\n",
    "\n",
    "def train(model_yaml, data_yaml, epochs=100, imgsz=640, batch=16, device='0'):\n",
    "    # Load model from architecture YAML\n",
    "    model = YOLO(model_yaml)\n",
    "\n",
    "    # Start training\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        device=device,\n",
    "        workers=8,\n",
    "        name=\"custom-yolo-train\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Architecture\\model_architecture\\my_model.yaml\", help=\"Path to custom model YAML\")\n",
    "    parser.add_argument(\"--data\", type=str, default=\"data.yaml\", help=\"Path to dataset config YAML\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--imgsz\", type=int, default=640)\n",
    "    parser.add_argument(\"--batch\", type=int, default=16)\n",
    "    parser.add_argument(\"--device\", type=str, default='0')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(\n",
    "        model_yaml=args.model,\n",
    "        data_yaml=args.data,\n",
    "        epochs=args.epochs,\n",
    "        imgsz=args.imgsz,\n",
    "        batch=args.batch,\n",
    "        device=args.device\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
